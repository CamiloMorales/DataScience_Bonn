{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame, read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('./train.csv').as_matrix().astype(np.float32)\n",
    "\n",
    "np.random.shuffle(train)\n",
    "#val = train[0:12000,:]\n",
    "#train = train[12000:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convertOneHotEncoding(y):\n",
    "    oneHot = np.zeros([y.shape[0], 10])\n",
    "    for idx, l in enumerate(y):\n",
    "        oneHot[idx, l] = 1\n",
    "    return oneHot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train = train[:,1:].astype(float)\n",
    "x_train = np.divide(x_train, 255.0)\n",
    "train_mean = np.mean(x_train, axis=0)\n",
    "train_mean = train_mean.reshape(1,784)\n",
    "for idx,x in enumerate(x_train):\n",
    "    x_train[idx] = x - train_mean\n",
    "\n",
    "y_train = convertOneHotEncoding(train[:,0]).astype(np.float32)\n",
    "\n",
    "#x_val = val[:,1:].astype(float)\n",
    "#y_val = convertOneHotEncoding(val[:,0]).astype(np.float32)\n",
    "\n",
    "#for idx,x in enumerate(x_val):\n",
    "#    x_val[idx] = x - train_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('./test.csv')\n",
    "x_test = np.divide(test.as_matrix().astype(np.float32), 255.0)\n",
    "\n",
    "for idx, x in enumerate(x_test):\n",
    "    x_test[idx] = x - train_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createVariable(name, shape):\n",
    "    return tf.get_variable(name, initializer = tf.truncated_normal(shape,stddev=0.1))\n",
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_image = tf.reshape(x, [-1,28,28,1])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "#h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "with tf.variable_scope(\"layer1\"):\n",
    "    W_1 = createVariable(\"weights\", [5,5,1,8])\n",
    "    b_1 = createVariable(\"biases\", [8])\n",
    "    h_1 = tf.nn.relu(conv2d(x_image, W_1) + b_1)\n",
    "    \n",
    "with tf.variable_scope(\"layer2\"):\n",
    "    W_2 = createVariable(\"weights\", [5,5,8,16])\n",
    "    b_2 = createVariable(\"biases\", [16])\n",
    "    h_2 = tf.nn.relu(conv2d(h_1, W_2) + b_2)\n",
    "    \n",
    "with tf.variable_scope(\"layer3\"):\n",
    "    W_3 = createVariable(\"weights\", [5,5,16,24])\n",
    "    b_2 = createVariable(\"biases\", [24])\n",
    "    h_3 = tf.nn.relu(conv2d(h_2, W_3) + b_2)\n",
    "    \n",
    "h_4 = max_pool_2x2(h_3)\n",
    "\n",
    "with tf.variable_scope(\"layer5\"):\n",
    "    W_5 = createVariable(\"weights\", [5,5,24,24])\n",
    "    b_5 = createVariable(\"biases\", [24])\n",
    "    h_5 = tf.nn.relu(conv2d(h_4,W_5) + b_5)\n",
    "    \n",
    "with tf.variable_scope(\"layer6\"):\n",
    "    W_6 = createVariable(\"weights\", [5,5,24,24])\n",
    "    b_6 = createVariable(\"biases\", [24])\n",
    "    h_6 = tf.nn.relu(conv2d(h_5,W_6) + b_6)\n",
    "    \n",
    "with tf.variable_scope(\"layer7\"):\n",
    "    W_7 = createVariable(\"weights\", [5,5,24,24])\n",
    "    b_7 = createVariable(\"biases\", [24])\n",
    "    h_7 = tf.nn.relu(conv2d(h_6,W_7) + b_7)\n",
    "    \n",
    "h_8 = max_pool_2x2(h_7)\n",
    "\n",
    "with tf.variable_scope(\"layer9\"):\n",
    "    W_9 = createVariable(\"weights\", [3,3,24,32])\n",
    "    b_9 = createVariable(\"biases\", [32])\n",
    "    h_9 = tf.nn.relu(conv2d(h_8,W_9) + b_9)\n",
    "    \n",
    "with tf.variable_scope(\"layer10\"):\n",
    "    W_10 = createVariable(\"weights\", [3,3,32,40])\n",
    "    b_10 = createVariable(\"biases\", [40])\n",
    "    h_10 = tf.nn.relu(conv2d(h_9,W_10) + b_10)\n",
    "    \n",
    "with tf.variable_scope(\"layer11\"):\n",
    "    W_11 = createVariable(\"weights\", [3,3,40,48])\n",
    "    b_11 = createVariable(\"biases\", [48])\n",
    "    h_11 = tf.nn.relu(conv2d(h_10,W_11) + b_11)\n",
    "    \n",
    "h_12 = tf.reshape(max_pool_2x2(h_11), [-1, 4*4*48])\n",
    "\n",
    "with tf.variable_scope(\"layer12\"):\n",
    "    W_12 = createVariable(\"weights\", [4*4*48, 512])\n",
    "    b_12 = createVariable(\"biases\", [512])\n",
    "    h_13 = tf.nn.dropout(tf.nn.relu(tf.matmul(h_12, W_12) + b_12), keep_prob)\n",
    "    \n",
    "with tf.variable_scope(\"layer13\"):\n",
    "    W_o = createVariable(\"weights\", [512, 10])\n",
    "    b_o = createVariable(\"biases\", [10])\n",
    "    h = tf.nn.softmax(tf.matmul(h_13, W_o) + b_o)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y * tf.log(h), reduction_indices=[1]))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(h,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "prediction = tf.argmax(h,1)\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start = 0\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    for i in range(25000):\n",
    "        batch_x = x_train[start:(start+42),:]\n",
    "        batch_y = y_train[start:(start+42),:]\n",
    "        start = (start+42)%42000\n",
    "\n",
    "        if i%500 == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={x:batch_x, y:batch_y, keep_prob:1.0})\n",
    "            print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "            save_path = saver.save(sess, \"/home/camilo/kaggle_DataScience_Bonn/DataScience_Bonn.git/Digits/model/model.ckpt\")\n",
    "            print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "        else:\n",
    "            sess.run(train_step, feed_dict={x:batch_x, y:batch_y, keep_prob:0.5})\n",
    "    \n",
    "    save_path = saver.save(sess, \"/home/camilo/kaggle_DataScience_Bonn/DataScience_Bonn.git/Digits/model/model.ckpt\")\n",
    "    print(\"Model saved in file: %s\" % save_path)\n",
    "    \n",
    "    #print sess.run(accuracy, feed_dict={x:x_val, y:y_val})\n",
    "    #np.savetxt('result.txt',sess.run(prediction, feed_dict={x:x_test, keep_prob:1.0}), fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp_list = []\n",
    "\n",
    "with tf.Session() as sess: \n",
    "      saver.restore(sess, \"/home/camilo/kaggle_DataScience_Bonn/DataScience_Bonn.git/Digits/model/model.ckpt\")\n",
    "      for test in x_test:\n",
    "            test = np.reshape(test, [1,784])\n",
    "            temp_list.append(prediction.eval(feed_dict={x:test, keep_prob:1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#res = np.loadtxt('result.txt')\n",
    "\n",
    "lis = []\n",
    "for idx, y in enumerate(temp_list):\n",
    "    lis.append([idx+1,y])\n",
    "lis = np.array(lis)\n",
    "np.savetxt('result2.txt', lis, fmt='%d', delimiter=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
